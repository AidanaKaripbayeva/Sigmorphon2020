{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://towardsdatascience.com/pairwise-sequence-alignment-using-biopython-d1a9d0ba861f\n",
    "#NW in paper https://www.aclweb.org/anthology/W16-2002.pdf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from collections import Counter\n",
    "sys.path.insert(1, '/home/aidana/TurkicSigmorphon2020/code')\n",
    "\n",
    "from data import uniread\n",
    "\n",
    "from operator import itemgetter\n",
    "from itertools import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_table():\n",
    "    columns = ['N', 'PROPN', 'ADJ', 'PRO','PRE','CLF','ART','DET',\n",
    "               'V','ADV','AUX','V.AGT','V.PTCP','V.PTCP.PST','V.MSDR','V.CVB','V.CVB.GEN','V.CVB.SIM',\n",
    "               'ADP','COMP','CONJ','NUM','PART','INTJ', 'Total']\n",
    "    rows = ['Suffix', 'Prefix', 'Infix','Circumfix','Transfix','Unchanged', 'Fail to find']\n",
    "\n",
    "    #df[columns][rows][0] <- how lemma is different from stem\n",
    "    #df[columns][rows][1] <- how form is different from stem\n",
    "\n",
    "    zero_data = [[[0,0] for i in range(len(columns))] for j in range(len(rows))]\n",
    "    df = pd.DataFrame(zero_data,index=rows, columns=columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def findOccurrences_no_equal(s, ch):\n",
    "    return [i for i, letter in enumerate(s) if letter != ch]\n",
    "\n",
    "def findOccurrences(s, ch):\n",
    "    return [i for i, letter in enumerate(s) if letter == ch]\n",
    "\n",
    "def MaxLength(lst): \n",
    "    maxList = max(lst, key = len) \n",
    "    maxLength = max(map(len, lst)) \n",
    "      \n",
    "    return maxList, maxLength \n",
    "\n",
    "def my_format_alignment(align1, align2, score, begin, end): \n",
    "    s = []\n",
    "    match = 0\n",
    "    gap=0\n",
    "    mismatch=0\n",
    "    for a, b in zip(align1[begin:end], align2[begin:end]): \n",
    "        if a == b: \n",
    "            s.append('|')\n",
    "            match+=1  \n",
    "        elif a == \"-\" or b == \"-\": \n",
    "            s.append(' ')\n",
    "            gap+=1\n",
    "        else: \n",
    "            s.append('.')\n",
    "            mismatch+=1\n",
    "\n",
    "    return match,mismatch,gap,s\n",
    "\n",
    "def NW_algorithm(lemma,form):\n",
    "    al = []\n",
    "    alignments = pairwise2.align.globalms(lemma, form, 2, -1, -1, -0.1)\n",
    "    #for a in alignments:\n",
    "     #   print(format_alignment(*a)) \n",
    "\n",
    "    match, mismatch, gap, al = my_format_alignment(*alignments[0])\n",
    "    #print('Matches = ', match, ', Mismatch = ', mismatch, ', Gap = ', gap)\n",
    "    #print(al)\n",
    "    \n",
    "    return match, mismatch, gap, al, alignments\n",
    "\n",
    "def finding_stem_list(al):\n",
    "    pos = findOccurrences(al, '|')   \n",
    "    groups = []\n",
    "    for r, g in groupby(enumerate(pos), lambda x: x[0]-x[1]):\n",
    "        groups.append(list(map(itemgetter(1), g)))\n",
    "\n",
    "    stem_list = []\n",
    "    for i in groups:\n",
    "        if (len(i) >= k):\n",
    "            stem_list.append(i)\n",
    "    \n",
    "    return stem_list\n",
    "\n",
    "\n",
    "def update_table_with_inf(word, stems, item, df, tag):    \n",
    "    first_stem_pos = word.find(stems[0])\n",
    "    last_stem_pos = word.find(stems[-1])\n",
    "    \n",
    "    if(first_stem_pos > 0 or last_stem_pos+len(stems[-1]) < len(word)):\n",
    "        df[tag]['Transfix'][item] = df[tag]['Transfix'][item] + 1\n",
    "        #print(\"df[tag]['Transfix']\")\n",
    "    else:\n",
    "        df[tag]['Infix'][item] = df[tag]['Infix'][item] + 1\n",
    "        #print(\"df[tag]['Infix']\")\n",
    "    \n",
    "    \n",
    "def update_table_no_inf(word, stem, item, df, tag):\n",
    "    stem_pos = word.find(stem) \n",
    "    if(stem_pos > 0 and stem_pos+len(stem) < len(word)):\n",
    "        df[tag]['Circumfix'][item] = df[tag]['Circumfix'][item] + 1\n",
    "        #print('df[tag][Circumfix]')\n",
    "    elif (stem_pos == 0 and stem_pos+len(stem) < len(word)):\n",
    "        df[tag]['Suffix'][item] = df[tag]['Suffix'][item] + 1\n",
    "        #print(\"df[tag]['Suffix']\")\n",
    "    elif (stem_pos > 0 and stem_pos+len(stem) == len(word)):\n",
    "        df[tag]['Prefix'][item] = df[tag]['Prefix'][item] + 1\n",
    "        #print(\"df[tag]['Prefix']\")\n",
    "    else:\n",
    "        df[tag]['Unchanged'][item] = df[tag]['Unchanged'][item] + 1\n",
    "        #print(\"df[tag]['Unchanged']\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cebtrn= uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/austronesian/ceb.trn\",2,1)\n",
    "hiltrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/austronesian/hil.trn\",2,2)\n",
    "maotrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/austronesian/mao.trn\",2,3)\n",
    "mlgtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/austronesian/mlg.trn\",2,4)\n",
    "tgltrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/austronesian/tgl.trn\",2,5)\n",
    "\n",
    "angtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/germanic/ang.trn\",1,1)\n",
    "dantrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/germanic/dan.trn\",1,2)\n",
    "deutrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/germanic/deu.trn\",1,3)\n",
    "engtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/germanic/eng.trn\",1,4)\n",
    "frrtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/germanic/frr.trn\",1,5)\n",
    "gmhtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/germanic/gmh.trn\",1,6)\n",
    "isltrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/germanic/isl.trn\",1,7)\n",
    "nldtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/germanic/nld.trn\",1,8)\n",
    "nobtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/germanic/nob.trn\",1,9)\n",
    "swetrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/germanic/swe.trn\",1,10)\n",
    "\n",
    "\n",
    "akatrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/niger-congo/aka.trn\",3,1)\n",
    "gaatrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/niger-congo/gaa.trn\",3,2)\n",
    "kontrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/niger-congo/kon.trn\",3,3)\n",
    "lintrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/niger-congo/lin.trn\",3,4)\n",
    "lugtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/niger-congo/lug.trn\",3,5)\n",
    "nyatrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/niger-congo/nya.trn\",3,6)\n",
    "sottrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/niger-congo/sot.trn\",3,7)\n",
    "swatrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/niger-congo/swa.trn\",3,8)\n",
    "zultrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/niger-congo/zul.trn\",3,9)\n",
    "\n",
    "\n",
    "azgtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/oto-manguean/azg.trn\",4,1)\n",
    "clytrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/oto-manguean/cly.trn\",4,2)\n",
    "cpatrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/oto-manguean/cpa.trn\",4,3)\n",
    "ctptrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/oto-manguean/ctp.trn\",4,4)\n",
    "czntrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/oto-manguean/czn.trn\",4,5)\n",
    "otetrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/oto-manguean/ote.trn\",4,6)\n",
    "otmtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/oto-manguean/otm.trn\",4,7)\n",
    "peitrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/oto-manguean/pei.trn\",4,8)\n",
    "xtytrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/oto-manguean/xty.trn\",4,9)\n",
    "zpvtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/oto-manguean/zpv.trn\",4,10)\n",
    "\n",
    "esttrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/uralic/est.trn\",5,1)\n",
    "fintrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/uralic/fin.trn\",5,2)\n",
    "izhtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/uralic/izh.trn\",5,3)\n",
    "krltrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/uralic/krl.trn\",5,4)\n",
    "livtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/uralic/liv.trn\",5,5)\n",
    "mdftrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/uralic/mdf.trn\",5,6)\n",
    "mhrtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/uralic/mhr.trn\",5,7)\n",
    "myvtrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/uralic/myv.trn\",5,8)\n",
    "smetrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/uralic/sme.trn\",5,9)\n",
    "veptrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/uralic/vep.trn\",5,10)\n",
    "vottrn = uniread.read_unimorph_tsv(\"/home/aidana/sigmorphon2020/uralic/vot.trn\",5,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "austronesian = [cebtrn, hiltrn, maotrn, mlgtrn,tgltrn]\n",
    "austronesian_names = ['cebtrn', 'hiltrn', 'maotrn', 'mlgtrn','tgltrn']\n",
    "germanic = [angtrn, dantrn, deutrn, engtrn, frrtrn, gmhtrn, isltrn, nldtrn, nobtrn, swetrn]\n",
    "germanic_names = ['angtrn', 'dantrn', 'deutrn', 'engtrn', 'frrtrn', 'gmhtrn', 'isltrn', 'nldtrn', 'nobtrn', 'swetrn']\n",
    "niger_congo = [akatrn, gaatrn, kontrn, lintrn, lugtrn, nyatrn, sottrn, swatrn, zultrn]\n",
    "niger_congo_names = ['akatrn', 'gaatrn', 'kontrn', 'lintrn', 'lugtrn', 'nyatrn', 'sottrn', 'swatrn', 'zultrn']\n",
    "oto_manguean = [azgtrn, clytrn, cpatrn, ctptrn, czntrn, otetrn, otmtrn, peitrn, xtytrn, zpvtrn]\n",
    "oto_manguean_names = ['azgtrn', 'clytrn', 'cpatrn', 'ctptrn', 'czntrn', 'otetrn', 'otmtrn', 'peitrn', 'xtytrn', 'zpvtrn']\n",
    "uralic = [esttrn, fintrn,izhtrn,krltrn,livtrn,mdftrn,mhrtrn,myvtrn,smetrn,veptrn,vottrn]\n",
    "uralic_names = ['esttrn', 'fintrn','izhtrn','krltrn','livtrn','mdftrn','mhrtrn','myvtrn','smetrn','veptrn','vottrn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = [austronesian, germanic, niger_congo, oto_manguean, uralic]\n",
    "families_string = ['austronesian', 'germanic', 'niger_congo', 'oto_manguean', 'uralic']\n",
    "families_names = [austronesian_names, germanic_names, niger_congo_names, oto_manguean_names, uralic_names]\n",
    "\n",
    "for count_fam,fam in enumerate(families):\n",
    "    for count_lang, lang in enumerate(fam):\n",
    "        lang.to_csv('languages/' + families_string[count_fam] + '/' + families_names[count_fam][count_lang]+'.csv', sep = ',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k = 3\n",
    "#for fam_name in families_names:\n",
    " #   for lang_name in fam_name:\n",
    "        \n",
    "k = 3 \n",
    "austronesian_names = ['cebtrn', 'hiltrn', 'maotrn', 'mlgtrn','tgltrn']\n",
    "germanic_names = ['angtrn', 'dantrn', 'deutrn', 'engtrn', 'frrtrn', 'gmhtrn', 'isltrn', 'nldtrn', 'nobtrn', 'swetrn']\n",
    "niger_congo_names = ['akatrn', 'gaatrn', 'kontrn', 'lintrn', 'lugtrn', 'nyatrn', 'sottrn', 'swatrn', 'zultrn']\n",
    "oto_manguean_names = ['azgtrn', 'clytrn', 'cpatrn', 'ctptrn', 'czntrn', 'otetrn', 'otmtrn', 'peitrn', 'xtytrn', 'zpvtrn']\n",
    "uralic_names = ['esttrn', 'fintrn','izhtrn','krltrn','livtrn','mdftrn','mhrtrn','myvtrn','smetrn','veptrn','vottrn']\n",
    "\n",
    "families_string = ['austronesian', 'germanic', 'niger_congo', 'oto_manguean', 'uralic']\n",
    "families_names = [austronesian_names, germanic_names, niger_congo_names, oto_manguean_names, uralic_names]\n",
    "\n",
    "filename = 'affix_analysis_k3.csv'\n",
    "beginning = ['This is an analysis of affixes for Sigmorphon2020']\n",
    "pd.DataFrame(beginning).T.to_csv(filename, index=False, header=False)\n",
    "\n",
    "for count_fam,fam in enumerate(families_names):\n",
    "    family_language = ['Family language => ' + families_string[count_fam]]\n",
    "    with open(filename, 'a') as f: \n",
    "            pd.DataFrame(family_language).to_csv(f, index=False, header=False)\n",
    "    for count_lang, lang in enumerate(fam):    \n",
    "        result_to_file = []\n",
    "        result_to_file = ['Affix analysis table for ' + lang]\n",
    "        dataset = pd.read_csv('languages/' + families_string[count_fam] + '/' + lang +'.csv')\n",
    "        df = new_table()\n",
    "        t1 = time.time()\n",
    "        for i in range(dataset.shape[0]):\n",
    "\n",
    "            lemma = str(dataset['lemma'][i]) #damdam\n",
    "            form = str(dataset['form'][i]) #mandaramdam\n",
    "            tag = str(dataset['tags'][i]).partition(\";\")[0]\n",
    "\n",
    "\n",
    "\n",
    "            match, mismatch, gap, al, alignments = NW_algorithm(lemma, form)\n",
    "\n",
    "\n",
    "\n",
    "            stem_list = finding_stem_list(al)\n",
    "            if(match < k or len(stem_list)==0):\n",
    "                df[tag]['Fail to find'][0] = df[tag]['Fail to find'][0] + 1\n",
    "                df[tag]['Fail to find'][1] = df[tag]['Fail to find'][1] + 1\n",
    "                #print('Fail to find')\n",
    "                continue\n",
    "\n",
    "\n",
    "            if len(stem_list) > 1:\n",
    "                stems =[]\n",
    "                for count,i in enumerate(stem_list):\n",
    "                    stems.append(alignments[0][0][stem_list[count][0]:stem_list[count][-1]+1])\n",
    "\n",
    "                update_table_with_inf(lemma, stems, 0, df, tag) \n",
    "                update_table_with_inf(form, stems, 1, df, tag)  \n",
    "                #print(lemma, form, stems)\n",
    "\n",
    "            else:\n",
    "                stem = alignments[0][0][stem_list[0][0]:stem_list[0][-1]+1]\n",
    "                update_table_no_inf(lemma, stem, 0, df, tag)\n",
    "                update_table_no_inf(form, stem, 1, df, tag)\n",
    "                #print(lemma, form, stem)\n",
    "\n",
    "\n",
    "        t2 = time.time()\n",
    "        hours, rem = divmod(t2-t1, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print (\"It took \", hours, \"hours, \", minutes, \"minutes, \", seconds, \"seconds to finish this task\")\n",
    "\n",
    "        with open(filename, 'a') as f: \n",
    "            pd.DataFrame(result_to_file).to_csv(f, index=False, header=False)\n",
    "            df.to_csv(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
